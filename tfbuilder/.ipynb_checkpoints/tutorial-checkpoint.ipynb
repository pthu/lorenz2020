{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tfbuilder tutorial\n",
    "The whole machinery of tfbuilder can be used by importing the convert function from the tfbuilder library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfbuilder import convert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### arguments `convert()`\n",
    "    input_path      = folder path in which the files to be converted reside\n",
    "    output_path     = folder path to which all tf-modules are to be written\n",
    "    tlg_out         = `True` if one wants TLG codes as folder names `False` if folder names from metadata\n",
    "    ignore_empty    = `True` if source files that don't produce slot numbers need to be ignored\n",
    "    generic         = Generic metadata to be present in every tf-file to be produced\n",
    "    lang            = language (referring to languages available in `langsettings`)\n",
    "    typ             = subtype of a language, if special behavious is required, like `tlge` (tlg-e cdrom)\n",
    "    header          = if True, the convertor expects csv-files to have a header\n",
    "    version         = version number to be assigned to the tf-module\n",
    "    langsettings    = langsettings to be imported; usually, this is the langsettings provided by tfbuilder\n",
    "    multiprocessing = False --> no multiprocessing\n",
    "                    = True  --> active multiprocessing; authomatic assignment of number of processor threads\n",
    "                    = int   --> manual assingment of number of processor threads\n",
    "    chunksize       = number of files to be assigned to each thread each cycle\n",
    "    inspect         = return useful information about tags and attributes of XML input to inspect the source\n",
    "    silent          = if True, all TF-messages are suppressed\n",
    "    \n",
    "    \n",
    "#### remarks `generic` and `langsettings`:\n",
    "Both are accessible and changeable in tf_config.py. However, one is able to pass his/her own settings (=dictionary) to the convert function..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s parsing /home/ernstboogert/github/pthu/lorenz2020/tfbuilder/test/english/tlg0031.tlg004.perseus-eng2.xml\n",
      "This is Text-Fabric 7.10.0\n",
      "Api reference : https://annotation.github.io/text-fabric/Api/Fabric/\n",
      "\n",
      "0 features found and 0 ignored\n",
      "  0.00s Warp feature \"otype\" not found in\n",
      "/home/ernstboogert/github/pthu/lorenz2020/tfbuilder/test/english/out/Rainbow Missions, Inc/World English Bible - John, Machine readable text/Rainbow Missions, Inc/1/tf/1.0/\n",
      "  0.00s Warp feature \"oslots\" not found in\n",
      "/home/ernstboogert/github/pthu/lorenz2020/tfbuilder/test/english/out/Rainbow Missions, Inc/World English Bible - John, Machine readable text/Rainbow Missions, Inc/1/tf/1.0/\n",
      "  0.01s Warp feature \"otext\" not found. Working without Text-API\n",
      "\n",
      "  0.00s Importing data from walking through the source ...\n",
      "   |     0.00s Preparing metadata... \n",
      "   |   SECTION   TYPES:    chapter, verse\n",
      "   |   SECTION   FEATURES: chapter, verse\n",
      "   |   STRUCTURE TYPES:    _book, chapter, verse, _sentence, _phrase\n",
      "   |   STRUCTURE FEATURES: _book, chapter, verse, _sentence, _phrase\n",
      "   |   TEXT      FEATURES:\n",
      "   |      |   text-orig-full       orig, post, pre\n",
      "   |      |   text-orig-main       main\n",
      "   |      |   text-orig-plain      plain\n",
      "   |     0.01s OK\n",
      "   |     0.00s Following director... \n",
      "   |     0.62s \"edge\" actions: 0\n",
      "   |     0.62s \"feature\" actions: 101225\n",
      "   |     0.62s \"node\" actions: 6308\n",
      "   |     0.63s \"resume\" actions: 1778\n",
      "   |     0.63s \"slot\" actions: 18975\n",
      "   |     0.63s \"terminate\" actions: 9951\n",
      "   |          1 x \"_book\" node \n",
      "   |       3304 x \"_phrase\" node \n",
      "   |       1220 x \"_sentence\" node \n",
      "   |         22 x \"chapter\" node \n",
      "   |          1 x \"edition-eng\" node \n",
      "   |          1 x \"head\" node \n",
      "   |        879 x \"p\" node \n",
      "   |        880 x \"verse\" node \n",
      "   |      18975 x \"word\" node  = slot type\n",
      "   |      25283 nodes of all types\n",
      "   |     0.64s OK\n",
      "   |     0.00s Removing unlinked nodes ... \n",
      "   |      |    -0.00s      1 unlinked \"_sentence\" node: [1220]\n",
      "   |      |     0.00s      1 unlinked \"_phrase\" node: [3304]\n",
      "   |      |     0.00s      2 unlinked nodes\n",
      "   |      |     0.00s Leaving  25281 nodes\n",
      "   |     0.00s checking for nodes and edges ... \n",
      "   |     0.00s OK\n",
      "   |     0.00s checking features ... \n",
      "   |     0.00s OK\n",
      "   |     0.00s reordering nodes ...\n",
      "   |     0.05s Sorting 1 nodes of type \"_book\"\n",
      "   |     0.05s Sorting 3303 nodes of type \"_phrase\"\n",
      "   |     0.06s Sorting 1219 nodes of type \"_sentence\"\n",
      "   |     0.07s Sorting 22 nodes of type \"chapter\"\n",
      "   |     0.08s Sorting 1 nodes of type \"edition-eng\"\n",
      "   |     0.08s Sorting 1 nodes of type \"head\"\n",
      "   |     0.08s Sorting 879 nodes of type \"p\"\n",
      "   |     0.09s Sorting 880 nodes of type \"verse\"\n",
      "   |     0.10s Max node = 25281\n",
      "   |     0.10s OK\n",
      "   |     0.00s reassigning feature values ...\n",
      "   |      |     0.12s node feature \"_book\" with 1 node\n",
      "   |      |     0.12s node feature \"_phrase\" with 3303 nodes\n",
      "   |      |     0.12s node feature \"_sentence\" with 1219 nodes\n",
      "   |      |     0.12s node feature \"chapter\" with 22 nodes\n",
      "   |      |     0.13s node feature \"edition-eng\" with 1 node\n",
      "   |      |     0.13s node feature \"head\" with 1 node\n",
      "   |      |     0.13s node feature \"main\" with 18975 nodes\n",
      "   |      |     0.14s node feature \"orig\" with 18975 nodes\n",
      "   |      |     0.15s node feature \"p\" with 879 nodes\n",
      "   |      |     0.15s node feature \"plain\" with 18975 nodes\n",
      "   |      |     0.16s node feature \"post\" with 18975 nodes\n",
      "   |      |     0.17s node feature \"pre\" with 18975 nodes\n",
      "   |      |     0.18s node feature \"verse\" with 880 nodes\n",
      "   |     0.07s OK\n",
      "  0.00s Exporting 14 node and 1 edge and 1 config features to /home/ernstboogert/github/pthu/lorenz2020/tfbuilder/test/english/out/Rainbow Missions, Inc/World English Bible - John, Machine readable text/Rainbow Missions, Inc/1/tf/1.0:\n",
      "  0.00s VALIDATING oslots feature\n",
      "  0.01s VALIDATING oslots feature\n",
      "  0.01s maxSlot=      18975\n",
      "  0.01s maxNode=      25281\n",
      "  0.01s OK: oslots is valid\n",
      "   |     0.00s T _book                to /home/ernstboogert/github/pthu/lorenz2020/tfbuilder/test/english/out/Rainbow Missions, Inc/World English Bible - John, Machine readable text/Rainbow Missions, Inc/1/tf/1.0\n",
      "   |     0.02s T _phrase              to /home/ernstboogert/github/pthu/lorenz2020/tfbuilder/test/english/out/Rainbow Missions, Inc/World English Bible - John, Machine readable text/Rainbow Missions, Inc/1/tf/1.0\n",
      "   |     0.01s T _sentence            to /home/ernstboogert/github/pthu/lorenz2020/tfbuilder/test/english/out/Rainbow Missions, Inc/World English Bible - John, Machine readable text/Rainbow Missions, Inc/1/tf/1.0\n",
      "   |     0.00s T chapter              to /home/ernstboogert/github/pthu/lorenz2020/tfbuilder/test/english/out/Rainbow Missions, Inc/World English Bible - John, Machine readable text/Rainbow Missions, Inc/1/tf/1.0\n",
      "   |     0.00s T edition-eng          to /home/ernstboogert/github/pthu/lorenz2020/tfbuilder/test/english/out/Rainbow Missions, Inc/World English Bible - John, Machine readable text/Rainbow Missions, Inc/1/tf/1.0\n",
      "   |     0.00s T head                 to /home/ernstboogert/github/pthu/lorenz2020/tfbuilder/test/english/out/Rainbow Missions, Inc/World English Bible - John, Machine readable text/Rainbow Missions, Inc/1/tf/1.0\n",
      "   |     0.05s T main                 to /home/ernstboogert/github/pthu/lorenz2020/tfbuilder/test/english/out/Rainbow Missions, Inc/World English Bible - John, Machine readable text/Rainbow Missions, Inc/1/tf/1.0\n",
      "   |     0.05s T orig                 to /home/ernstboogert/github/pthu/lorenz2020/tfbuilder/test/english/out/Rainbow Missions, Inc/World English Bible - John, Machine readable text/Rainbow Missions, Inc/1/tf/1.0\n",
      "   |     0.02s T otype                to /home/ernstboogert/github/pthu/lorenz2020/tfbuilder/test/english/out/Rainbow Missions, Inc/World English Bible - John, Machine readable text/Rainbow Missions, Inc/1/tf/1.0\n",
      "   |     0.00s T p                    to /home/ernstboogert/github/pthu/lorenz2020/tfbuilder/test/english/out/Rainbow Missions, Inc/World English Bible - John, Machine readable text/Rainbow Missions, Inc/1/tf/1.0\n",
      "   |     0.06s T plain                to /home/ernstboogert/github/pthu/lorenz2020/tfbuilder/test/english/out/Rainbow Missions, Inc/World English Bible - John, Machine readable text/Rainbow Missions, Inc/1/tf/1.0\n",
      "   |     0.05s T post                 to /home/ernstboogert/github/pthu/lorenz2020/tfbuilder/test/english/out/Rainbow Missions, Inc/World English Bible - John, Machine readable text/Rainbow Missions, Inc/1/tf/1.0\n",
      "   |     0.05s T pre                  to /home/ernstboogert/github/pthu/lorenz2020/tfbuilder/test/english/out/Rainbow Missions, Inc/World English Bible - John, Machine readable text/Rainbow Missions, Inc/1/tf/1.0\n",
      "   |     0.01s T verse                to /home/ernstboogert/github/pthu/lorenz2020/tfbuilder/test/english/out/Rainbow Missions, Inc/World English Bible - John, Machine readable text/Rainbow Missions, Inc/1/tf/1.0\n",
      "   |     0.07s T oslots               to /home/ernstboogert/github/pthu/lorenz2020/tfbuilder/test/english/out/Rainbow Missions, Inc/World English Bible - John, Machine readable text/Rainbow Missions, Inc/1/tf/1.0\n",
      "   |     0.00s M otext                to /home/ernstboogert/github/pthu/lorenz2020/tfbuilder/test/english/out/Rainbow Missions, Inc/World English Bible - John, Machine readable text/Rainbow Missions, Inc/1/tf/1.0\n",
      "  0.43s Exported 14 node features and 1 edge features and 1 config features to /home/ernstboogert/github/pthu/lorenz2020/tfbuilder/test/english/out/Rainbow Missions, Inc/World English Bible - John, Machine readable text/Rainbow Missions, Inc/1/tf/1.0\n",
      "  1.43s    |    Conversion of tlg0031.tlg004.perseus-eng2.xml was successful...!\n",
      "\n",
      "  1.44s 1 of 1 works have successfully been converted!\n"
     ]
    }
   ],
   "source": [
    "convert('~/github/pthu/lorenz2020/tfbuilder/test/english', \n",
    "        '~/github/pthu/lorenz2020/tfbuilder/test/english/out',\n",
    "        ignore_empty=False,\n",
    "        tlg_out=False,\n",
    "        lang='generic', \n",
    "        typ=False, \n",
    "        header=False,\n",
    "        multiprocessing=False,\n",
    "        chunksize=1,\n",
    "        silent=False,\n",
    "       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert csv file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation Homer (James Tauber)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1\\t1\\tμῆνιν ἄειδε θεὰ Πηληϊάδεω Ἀχιλῆος\\n',\n",
      " '1\\t2\\tοὐλομένην, ἣ μυρίʼ Ἀχαιοῖς ἄλγεʼ ἔθηκε,\\n',\n",
      " '1\\t3\\tπολλὰς δʼ ἰφθίμους ψυχὰς Ἄϊδι προΐαψεν\\n',\n",
      " '1\\t4\\tἡρώων, αὐτοὺς δὲ ἑλώρια τεῦχε κύνεσσιν\\n',\n",
      " '1\\t5\\tοἰωνοῖσί τε πᾶσι, Διὸς δʼ ἐτελείετο βουλή,\\n',\n",
      " '1\\t6\\tἐξ οὗ δὴ τὰ πρῶτα διαστήτην ἐρίσαντε\\n',\n",
      " '1\\t7\\tἈτρεΐδης τε ἄναξ ἀνδρῶν καὶ δῖος Ἀχιλλεύς.\\n',\n",
      " '1\\t8\\tτίς τʼ ἄρ σφωε θεῶν ἔριδι ξυνέηκε μάχεσθαι;\\n',\n",
      " '1\\t9\\tΛητοῦς καὶ Διὸς υἱός· ὃ γὰρ βασιλῆϊ χολωθεὶς\\n',\n",
      " '1\\t10\\tνοῦσον ἀνὰ στρατὸν ὄρσε κακήν, ὀλέκοντο δὲ λαοί,\\n']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "PATH = os.path.expanduser('~/github/pthu/lorenz2020/tfbuilder/test')\n",
    "\n",
    "csvfile = open(PATH + '/greek_csv/tlg0012-001.csv', 'w+')\n",
    "\n",
    "with open(os.path.expanduser(PATH + '/greek_csv/tlg0012-001.txt'), 'r') as james:\n",
    "    for line in james.readlines():\n",
    "        ref, text = line.split(' ', 1)\n",
    "        ref1, ref2 = ref.split('.')\n",
    "        csvfile.write(f'{ref1}\\t{ref2}\\t{text}')\n",
    "\n",
    "csvfile.close()\n",
    "\n",
    "with open(PATH + '/greek_csv/tlg0012-001.csv', 'r') as ernst:\n",
    "    pprint(ernst.readlines()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s parsing /home/ernstboogert/github/pthu/lorenz2020/tfbuilder/test/greek_csv/tlg0012-001.csv\n",
      "This is Text-Fabric 7.10.0\n",
      "Api reference : https://annotation.github.io/text-fabric/Api/Fabric/\n",
      "\n",
      "0 features found and 0 ignored\n",
      "  0.00s Warp feature \"otype\" not found in\n",
      "/home/ernstboogert/github/pthu/lorenz2020/tfbuilder/test/greek_csv/out/Homerus/Ilias/Allen, T.W./1/tf/1.0/\n",
      "  0.00s Warp feature \"oslots\" not found in\n",
      "/home/ernstboogert/github/pthu/lorenz2020/tfbuilder/test/greek_csv/out/Homerus/Ilias/Allen, T.W./1/tf/1.0/\n",
      "  0.00s Warp feature \"otext\" not found. Working without Text-API\n",
      "\n",
      "No header data could be found; please enter an appropriate header: book line text\n",
      "  0.00s Importing data from walking through the source ...\n",
      "   |     0.00s Preparing metadata... \n",
      "   |   SECTION   TYPES:    book, line\n",
      "   |   SECTION   FEATURES: book, line\n",
      "   |   STRUCTURE TYPES:    _book, book, line, _sentence, _phrase\n",
      "   |   STRUCTURE FEATURES: _book, book, line, _sentence, _phrase\n",
      "   |   TEXT      FEATURES:\n",
      "   |      |   text-orig-full       orig, post, pre\n",
      "   |      |   text-orig-lemma      lemma\n",
      "   |      |   text-orig-main       main\n",
      "   |      |   text-orig-plain      plain\n",
      "   |     0.01s OK\n",
      "   |     0.00s Following director... \n",
      "   |     6.19s \"edge\" actions: 0\n",
      "   |     6.19s \"feature\" actions: 706244\n",
      "   |     6.19s \"node\" actions: 34970\n",
      "   |     6.19s \"resume\" actions: 3\n",
      "   |     6.19s \"slot\" actions: 111878\n",
      "   |     6.20s \"terminate\" actions: 34978\n",
      "   |          1 x \"_book\" node \n",
      "   |      15432 x \"_phrase\" node \n",
      "   |       3831 x \"_sentence\" node \n",
      "   |         24 x \"book\" node \n",
      "   |      15682 x \"line\" node \n",
      "   |     111878 x \"word\" node  = slot type\n",
      "   |     146848 nodes of all types\n",
      "   |     6.25s OK\n",
      "   |     0.01s Removing unlinked nodes ... \n",
      "   |      |    -0.00s      1 unlinked \"_sentence\" node: [3831]\n",
      "   |      |     0.00s      1 unlinked \"_phrase\" node: [15432]\n",
      "   |      |     0.01s      2 unlinked nodes\n",
      "   |      |     0.01s Leaving 146846 nodes\n",
      "   |     0.00s checking for nodes and edges ... \n",
      "   |     0.00s OK\n",
      "   |     0.00s checking features ... \n",
      "   |     0.04s OK\n",
      "   |     0.00s reordering nodes ...\n",
      "   |     0.09s Sorting 1 nodes of type \"_book\"\n",
      "   |     0.09s Sorting 15431 nodes of type \"_phrase\"\n",
      "   |     0.15s Sorting 3830 nodes of type \"_sentence\"\n",
      "   |     0.16s Sorting 24 nodes of type \"book\"\n",
      "   |     0.18s Sorting 15682 nodes of type \"line\"\n",
      "   |     0.23s Max node = 146846\n",
      "   |     0.23s OK\n",
      "   |     0.00s reassigning feature values ...\n",
      "   |      |     0.32s node feature \"_book\" with 1 node\n",
      "   |      |     0.32s node feature \"_phrase\" with 15431 nodes\n",
      "   |      |     0.33s node feature \"_sentence\" with 3830 nodes\n",
      "   |      |     0.34s node feature \"book\" with 24 nodes\n",
      "   |      |     0.35s node feature \"lemma\" with 111878 nodes\n",
      "   |      |     0.41s node feature \"line\" with 15682 nodes\n",
      "   |      |     0.42s node feature \"main\" with 111878 nodes\n",
      "   |      |     0.47s node feature \"orig\" with 111878 nodes\n",
      "   |      |     0.51s node feature \"plain\" with 111878 nodes\n",
      "   |      |     0.56s node feature \"post\" with 111878 nodes\n",
      "   |      |     0.60s node feature \"pre\" with 111878 nodes\n",
      "   |     0.37s OK\n",
      "  0.00s Exporting 12 node and 1 edge and 1 config features to /home/ernstboogert/github/pthu/lorenz2020/tfbuilder/test/greek_csv/out/Homerus/Ilias/Allen, T.W./1/tf/1.0:\n",
      "  0.00s VALIDATING oslots feature\n",
      "  0.03s VALIDATING oslots feature\n",
      "  0.03s maxSlot=     111878\n",
      "  0.03s maxNode=     146846\n",
      "  0.04s OK: oslots is valid\n",
      "   |     0.00s T _book                to /home/ernstboogert/github/pthu/lorenz2020/tfbuilder/test/greek_csv/out/Homerus/Ilias/Allen, T.W./1/tf/1.0\n",
      "   |     0.06s T _phrase              to /home/ernstboogert/github/pthu/lorenz2020/tfbuilder/test/greek_csv/out/Homerus/Ilias/Allen, T.W./1/tf/1.0\n",
      "   |     0.02s T _sentence            to /home/ernstboogert/github/pthu/lorenz2020/tfbuilder/test/greek_csv/out/Homerus/Ilias/Allen, T.W./1/tf/1.0\n",
      "   |     0.00s T book                 to /home/ernstboogert/github/pthu/lorenz2020/tfbuilder/test/greek_csv/out/Homerus/Ilias/Allen, T.W./1/tf/1.0\n",
      "   |     0.28s T lemma                to /home/ernstboogert/github/pthu/lorenz2020/tfbuilder/test/greek_csv/out/Homerus/Ilias/Allen, T.W./1/tf/1.0\n",
      "   |     0.03s T line                 to /home/ernstboogert/github/pthu/lorenz2020/tfbuilder/test/greek_csv/out/Homerus/Ilias/Allen, T.W./1/tf/1.0\n",
      "   |     0.26s T main                 to /home/ernstboogert/github/pthu/lorenz2020/tfbuilder/test/greek_csv/out/Homerus/Ilias/Allen, T.W./1/tf/1.0\n",
      "   |     0.28s T orig                 to /home/ernstboogert/github/pthu/lorenz2020/tfbuilder/test/greek_csv/out/Homerus/Ilias/Allen, T.W./1/tf/1.0\n",
      "   |     0.07s T otype                to /home/ernstboogert/github/pthu/lorenz2020/tfbuilder/test/greek_csv/out/Homerus/Ilias/Allen, T.W./1/tf/1.0\n",
      "   |     0.29s T plain                to /home/ernstboogert/github/pthu/lorenz2020/tfbuilder/test/greek_csv/out/Homerus/Ilias/Allen, T.W./1/tf/1.0\n",
      "   |     0.20s T post                 to /home/ernstboogert/github/pthu/lorenz2020/tfbuilder/test/greek_csv/out/Homerus/Ilias/Allen, T.W./1/tf/1.0\n",
      "   |     0.22s T pre                  to /home/ernstboogert/github/pthu/lorenz2020/tfbuilder/test/greek_csv/out/Homerus/Ilias/Allen, T.W./1/tf/1.0\n",
      "   |     0.24s T oslots               to /home/ernstboogert/github/pthu/lorenz2020/tfbuilder/test/greek_csv/out/Homerus/Ilias/Allen, T.W./1/tf/1.0\n",
      "   |     0.00s M otext                to /home/ernstboogert/github/pthu/lorenz2020/tfbuilder/test/greek_csv/out/Homerus/Ilias/Allen, T.W./1/tf/1.0\n",
      "  2.03s Exported 12 node features and 1 edge features and 1 config features to /home/ernstboogert/github/pthu/lorenz2020/tfbuilder/test/greek_csv/out/Homerus/Ilias/Allen, T.W./1/tf/1.0\n",
      "    16s    |    Conversion was successful...\n",
      "\n",
      "    16s 1 of 1 works have successfully been converted!\n"
     ]
    }
   ],
   "source": [
    "convert('~/github/pthu/lorenz2020/tfbuilder/test/greek_csv', \n",
    "        '~/github/pthu/lorenz2020/tfbuilder/test/greek_csv/out',\n",
    "        ignore_empty=False,\n",
    "        tlg_out=False,\n",
    "        lang='greek', \n",
    "        typ='tlge', \n",
    "        header=False,\n",
    "        multiprocessing=False,\n",
    "        chunksize=1,\n",
    "        silent=False,\n",
    "       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `generic` in `tf_config`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "generic_metadata_modified = {\n",
    "    'convertor_execution': 'Donald Duck',             # Please replace this by your own name!\n",
    "    'convertor_author': 'Willie Wortel',              # Idem!\n",
    "    'convertor_date': 'February, 1816',               # Replace by appropriate date\n",
    "    'convertor_institution': 'The Duck University',   # Replace by your own institution/company\n",
    "    'famous_uncle': 'Dagobert Duck',\n",
    "    \n",
    "    # DO NOT CHANGE!\n",
    "    'convertor_version': '1.0.0',           # NO CHANGE!\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `langsettings` in `tf_config`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpertools import langtools\n",
    "from collections import OrderedDict\n",
    "\n",
    "langsettings_modified = {\n",
    "    'english': {\n",
    "        #OUTPUT DIR STRUCTURE\n",
    "        #Output dir struct; NB these variable names need to be defined in the metadata!\n",
    "        #Multiple items in the list define multiple options that will be checked from left to right\n",
    "        #Output author/title/editor (or one of the other options if they are not provided\n",
    "        'dir_struct': [['author', 'editor'], \n",
    "                       ['title', 'book', 'work'], \n",
    "                       ['editor']],\n",
    "        \n",
    "        #TF variables!\n",
    "        'slot_type': 'word',\n",
    "        'intFeatures': set(),\n",
    "        'nonIntFeatures': {'otype', 'oslots', 'otext'},\n",
    "        'struct_counter': OrderedDict([('_sentence', 1), ('_phrase', 1)]),\n",
    "        'struct_counter_metadata': {\n",
    "            '_sentence': f\"sentences defined by the following delimiters: {{{'.', '?', '!',}}}\",\n",
    "            '_phrase': f\"sentences defined by the following delimiters: {{{',', ';', ':',}}}\",\n",
    "        },\n",
    "        'generic': {}, # = Metadata used by TF\n",
    "        \n",
    "        #LANGUAGE VARIABLES\n",
    "        #Unicode norm\n",
    "        'udnorm': 'NFD',\n",
    "        #Package of langtools\n",
    "        'langtool': langtools.Generic,\n",
    "        'replace_func': langtools.Generic.replace,\n",
    "        #Tokenizer\n",
    "        'tokenizer': langtools.Generic.splitTokenize,\n",
    "        'tokenizer_args': {'punc': True, \n",
    "                           'clean': False,\n",
    "                           'splitters': None,\n",
    "                           'non_splitters': (\"-\", \"'\"),},\n",
    "        'token_out': OrderedDict([('pre', {'text': False, 'description': 'interpunction before word'}),\n",
    "                                 ('orig', {'text': True, 'description': 'the original format of the word without interpunction'}),\n",
    "                                 ('post', {'text': False, 'description': 'interpunction after word'}),\n",
    "                                 ]),\n",
    "        #Lemmatizer\n",
    "        'lemmatizer': None,\n",
    "        #Text formats\n",
    "        'text_formats': {'orig': {'otext_name': 'fmt:text-orig-full',\n",
    "                                  'format': '{pre}{orig}{post}',\n",
    "                                  'function': langtools.Generic.origWord,\n",
    "                                  'description': 'original format of the word including punctuation'},\n",
    "                         'main': {'otext_name': 'fmt:text-orig-main',\n",
    "                                  'format': '{main} ',\n",
    "                                  'function': langtools.Generic.mainWord,\n",
    "                                  'description': 'normalized format of the word excluding punctuation'},\n",
    "                         'plain': {'otext_name': 'fmt:text-orig-plain',\n",
    "                                   'format': '{plain} ',\n",
    "                                   'function': langtools.Generic.plainWord,\n",
    "                                   'description': 'plain format in lowercase'},\n",
    "                        },\n",
    "\n",
    "        #XML VARIABLES\n",
    "        #Define the fields from xml metadata that need to be preserved\n",
    "        # concat = True means that subfields are concatenated\n",
    "        # concat = False means that subfields get their own metadata entry\n",
    "        'xmlmetadata': {'titleStmt': {'concat': False, 'delimit': ', ', 'end': ''},\n",
    "                     'publicationStmt': {'concat': True, 'delimit': ', ', 'end': '.'},\n",
    "                     'sourceDesc': {'concat': True, 'delimit': ', ', 'end': '.'},\n",
    "                     'license': {'concat': True, 'delimit': ', ', 'end': '.'},\n",
    "                     'availability': {'concat': True, 'delimit': ', ', 'end': '.'},\n",
    "                    },\n",
    "        \n",
    "        #Define the tag in which sectioning can be found\n",
    "        'section_tags': {'div',},\n",
    "        #Define in which key the section name can be found\n",
    "        'section_keys': {'subtype'},\n",
    "        #Define attribute keys that are superfluous and need to be ignored\n",
    "        'ignore_attrib_keys': set(),\n",
    "        #Define attribute keys that do not contain a section name\n",
    "        'non_section_keys': set(),\n",
    "        #Define values that are no sections, although they are in the right key\n",
    "        'non_section_values': set(),\n",
    "        ##Define attribute values that are superfluous and need to be ignored\n",
    "        ##'ignore_attrib_values': set(),\n",
    "        #Define tags that contain text elements that need not to be processed as regular text but as features\n",
    "        'non_text_tags': set(),\n",
    "        #Define attributes that have values that are feature names (values will be calculated automatically)\n",
    "        'feature_attribs': {'corresp', 'source'},\n",
    "        #Define sentence delimiters to be counted by struct_counter\n",
    "        'sentence_delimit': {'.', '?', '!',},\n",
    "        #Define phrase delimiters to be counted by struct_counter\n",
    "        'phrase_delimit': {',', ';', ':',},\n",
    "    },\n",
    "}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s parsing /home/ernstboogert/github/pthu/lorenz2020/tfbuilder/test/english/tlg0031.tlg004.perseus-eng2.xml\n",
      "This is Text-Fabric 7.10.0\n",
      "Api reference : https://annotation.github.io/text-fabric/Api/Fabric/\n",
      "\n",
      "0 features found and 0 ignored\n",
      "  0.00s Warp feature \"otype\" not found in\n",
      "/home/ernstboogert/github/pthu/lorenz2020/tfbuilder/test/english/out/Rainbow Missions, Inc/World English Bible - John, Machine readable text/Rainbow Missions, Inc/2/tf/1.0/\n",
      "  0.00s Warp feature \"oslots\" not found in\n",
      "/home/ernstboogert/github/pthu/lorenz2020/tfbuilder/test/english/out/Rainbow Missions, Inc/World English Bible - John, Machine readable text/Rainbow Missions, Inc/2/tf/1.0/\n",
      "  0.00s Warp feature \"otext\" not found. Working without Text-API\n",
      "\n",
      "  0.00s Importing data from walking through the source ...\n",
      "   |     0.00s Preparing metadata... \n",
      "   |   SECTION   TYPES:    chapter, verse\n",
      "   |   SECTION   FEATURES: chapter, verse\n",
      "   |   STRUCTURE TYPES:    _book, chapter, verse, _sentence, _phrase\n",
      "   |   STRUCTURE FEATURES: _book, chapter, verse, _sentence, _phrase\n",
      "   |   TEXT      FEATURES:\n",
      "   |      |   text-orig-full       orig, post, pre\n",
      "   |      |   text-orig-main       main\n",
      "   |      |   text-orig-plain      plain\n",
      "   |     0.01s OK\n",
      "   |     0.00s Following director... \n",
      "   |     0.67s \"edge\" actions: 0\n",
      "   |     0.68s \"feature\" actions: 99975\n",
      "   |     0.68s \"node\" actions: 6308\n",
      "   |     0.68s \"resume\" actions: 1803\n",
      "   |     0.68s \"slot\" actions: 18715\n",
      "   |     0.68s \"terminate\" actions: 9976\n",
      "   |          1 x \"_book\" node \n",
      "   |       3304 x \"_phrase\" node \n",
      "   |       1220 x \"_sentence\" node \n",
      "   |         22 x \"chapter\" node \n",
      "   |          1 x \"edition-eng\" node \n",
      "   |          1 x \"head\" node \n",
      "   |        879 x \"p\" node \n",
      "   |        880 x \"verse\" node \n",
      "   |      18715 x \"word\" node  = slot type\n",
      "   |      25023 nodes of all types\n",
      "   |     0.70s OK\n",
      "   |     0.00s Removing unlinked nodes ... \n",
      "   |      |    -0.00s      1 unlinked \"_sentence\" node: [1220]\n",
      "   |      |     0.00s      1 unlinked \"_phrase\" node: [3304]\n",
      "   |      |     0.00s      2 unlinked nodes\n",
      "   |      |     0.00s Leaving  25021 nodes\n",
      "   |     0.00s checking for nodes and edges ... \n",
      "   |     0.00s OK\n",
      "   |     0.00s checking features ... \n",
      "   |     0.01s OK\n",
      "   |     0.00s reordering nodes ...\n",
      "   |     0.02s Sorting 1 nodes of type \"_book\"\n",
      "   |     0.02s Sorting 3303 nodes of type \"_phrase\"\n",
      "   |     0.03s Sorting 1219 nodes of type \"_sentence\"\n",
      "   |     0.03s Sorting 22 nodes of type \"chapter\"\n",
      "   |     0.04s Sorting 1 nodes of type \"edition-eng\"\n",
      "   |     0.04s Sorting 1 nodes of type \"head\"\n",
      "   |     0.05s Sorting 879 nodes of type \"p\"\n",
      "   |     0.06s Sorting 880 nodes of type \"verse\"\n",
      "   |     0.07s Max node = 25021\n",
      "   |     0.07s OK\n",
      "   |     0.00s reassigning feature values ...\n",
      "   |      |     0.09s node feature \"_book\" with 1 node\n",
      "   |      |     0.10s node feature \"_phrase\" with 3303 nodes\n",
      "   |      |     0.10s node feature \"_sentence\" with 1219 nodes\n",
      "   |      |     0.10s node feature \"chapter\" with 22 nodes\n",
      "   |      |     0.10s node feature \"edition-eng\" with 1 node\n",
      "   |      |     0.10s node feature \"head\" with 1 node\n",
      "   |      |     0.10s node feature \"main\" with 18715 nodes\n",
      "   |      |     0.11s node feature \"orig\" with 18715 nodes\n",
      "   |      |     0.12s node feature \"p\" with 879 nodes\n",
      "   |      |     0.12s node feature \"plain\" with 18715 nodes\n",
      "   |      |     0.13s node feature \"post\" with 18715 nodes\n",
      "   |      |     0.14s node feature \"pre\" with 18715 nodes\n",
      "   |      |     0.15s node feature \"verse\" with 880 nodes\n",
      "   |     0.07s OK\n",
      "  0.00s Exporting 14 node and 1 edge and 1 config features to /home/ernstboogert/github/pthu/lorenz2020/tfbuilder/test/english/out/Rainbow Missions, Inc/World English Bible - John, Machine readable text/Rainbow Missions, Inc/2/tf/1.0:\n",
      "  0.00s VALIDATING oslots feature\n",
      "  0.01s VALIDATING oslots feature\n",
      "  0.01s maxSlot=      18715\n",
      "  0.02s maxNode=      25021\n",
      "  0.02s OK: oslots is valid\n",
      "   |     0.00s T _book                to /home/ernstboogert/github/pthu/lorenz2020/tfbuilder/test/english/out/Rainbow Missions, Inc/World English Bible - John, Machine readable text/Rainbow Missions, Inc/2/tf/1.0\n",
      "   |     0.01s T _phrase              to /home/ernstboogert/github/pthu/lorenz2020/tfbuilder/test/english/out/Rainbow Missions, Inc/World English Bible - John, Machine readable text/Rainbow Missions, Inc/2/tf/1.0\n",
      "   |     0.00s T _sentence            to /home/ernstboogert/github/pthu/lorenz2020/tfbuilder/test/english/out/Rainbow Missions, Inc/World English Bible - John, Machine readable text/Rainbow Missions, Inc/2/tf/1.0\n",
      "   |     0.00s T chapter              to /home/ernstboogert/github/pthu/lorenz2020/tfbuilder/test/english/out/Rainbow Missions, Inc/World English Bible - John, Machine readable text/Rainbow Missions, Inc/2/tf/1.0\n",
      "   |     0.00s T edition-eng          to /home/ernstboogert/github/pthu/lorenz2020/tfbuilder/test/english/out/Rainbow Missions, Inc/World English Bible - John, Machine readable text/Rainbow Missions, Inc/2/tf/1.0\n",
      "   |     0.00s T head                 to /home/ernstboogert/github/pthu/lorenz2020/tfbuilder/test/english/out/Rainbow Missions, Inc/World English Bible - John, Machine readable text/Rainbow Missions, Inc/2/tf/1.0\n",
      "   |     0.05s T main                 to /home/ernstboogert/github/pthu/lorenz2020/tfbuilder/test/english/out/Rainbow Missions, Inc/World English Bible - John, Machine readable text/Rainbow Missions, Inc/2/tf/1.0\n",
      "   |     0.05s T orig                 to /home/ernstboogert/github/pthu/lorenz2020/tfbuilder/test/english/out/Rainbow Missions, Inc/World English Bible - John, Machine readable text/Rainbow Missions, Inc/2/tf/1.0\n",
      "   |     0.02s T otype                to /home/ernstboogert/github/pthu/lorenz2020/tfbuilder/test/english/out/Rainbow Missions, Inc/World English Bible - John, Machine readable text/Rainbow Missions, Inc/2/tf/1.0\n",
      "   |     0.00s T p                    to /home/ernstboogert/github/pthu/lorenz2020/tfbuilder/test/english/out/Rainbow Missions, Inc/World English Bible - John, Machine readable text/Rainbow Missions, Inc/2/tf/1.0\n",
      "   |     0.05s T plain                to /home/ernstboogert/github/pthu/lorenz2020/tfbuilder/test/english/out/Rainbow Missions, Inc/World English Bible - John, Machine readable text/Rainbow Missions, Inc/2/tf/1.0\n",
      "   |     0.05s T post                 to /home/ernstboogert/github/pthu/lorenz2020/tfbuilder/test/english/out/Rainbow Missions, Inc/World English Bible - John, Machine readable text/Rainbow Missions, Inc/2/tf/1.0\n",
      "   |     0.03s T pre                  to /home/ernstboogert/github/pthu/lorenz2020/tfbuilder/test/english/out/Rainbow Missions, Inc/World English Bible - John, Machine readable text/Rainbow Missions, Inc/2/tf/1.0\n",
      "   |     0.00s T verse                to /home/ernstboogert/github/pthu/lorenz2020/tfbuilder/test/english/out/Rainbow Missions, Inc/World English Bible - John, Machine readable text/Rainbow Missions, Inc/2/tf/1.0\n",
      "   |     0.05s T oslots               to /home/ernstboogert/github/pthu/lorenz2020/tfbuilder/test/english/out/Rainbow Missions, Inc/World English Bible - John, Machine readable text/Rainbow Missions, Inc/2/tf/1.0\n",
      "   |     0.00s M otext                to /home/ernstboogert/github/pthu/lorenz2020/tfbuilder/test/english/out/Rainbow Missions, Inc/World English Bible - John, Machine readable text/Rainbow Missions, Inc/2/tf/1.0\n",
      "  0.36s Exported 14 node features and 1 edge features and 1 config features to /home/ernstboogert/github/pthu/lorenz2020/tfbuilder/test/english/out/Rainbow Missions, Inc/World English Bible - John, Machine readable text/Rainbow Missions, Inc/2/tf/1.0\n",
      "  1.39s    |    Conversion of tlg0031.tlg004.perseus-eng2.xml was successful...!\n",
      "\n",
      "  1.40s 1 of 1 works have successfully been converted!\n"
     ]
    }
   ],
   "source": [
    "convert('~/github/pthu/lorenz2020/tfbuilder/test/english', \n",
    "        '~/github/pthu/lorenz2020/tfbuilder/test/english/out',\n",
    "        generic=generic_metadata_modified,\n",
    "        langsettings=langsettings_modified,\n",
    "        ignore_empty=False,\n",
    "        tlg_out=False,\n",
    "        lang='english', \n",
    "        typ=False, \n",
    "        header=False,\n",
    "        multiprocessing=False,\n",
    "        chunksize=1,\n",
    "        silent=False,\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
